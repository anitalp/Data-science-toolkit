Decision trees & Random forest
========================

###Importante:

**Decision trees:**

- Admite gran número de variables
- Variables discretas y continuas
- No paramétrico
- NA tolerados

**Random forest:**
- Además de lo anterior, añade aleatoreidad a la muestra y a la construcción del modelo.


Trees can overfit if the number of trees is to high, so... prune your tree to reduce the number of subtrees.

#### Conceptos:


* **Pruning** is a technique in machine learning that reduces the size of decision trees by removing sections of the tree that provide little power to classify instances. Pruning reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting.

####Aplicaciones:

1. Clasificación y segmentación 
2. Selección de features
3. Toma de decisiones (dato agregado, con probabilidad)
En este último se pueden simular probabilidades con Montecarlo para evaluer si el camimo que estoy siguiendo es el más óptimo pese a cambiar mi predicción de probabilidades.

exito: 0.70, 0.76, 0.80, ....    10% arriba/abajo pero el tronco/rama/camino sigue siendo la mejor solución (ver ejemplos Gio)

Cuadrados: nodos de decisión
Redondos: nodos probabilísticos (multiplar el resultado por todas las probabilidades del camino que siga)



###Tipos de árboles (c50, part, etc etc)

Trees in R:

rpart (CART)
tree (CART)
ctree (conditional inference tree)
CHAID (chi-squared automatic interaction detection)
evtree (evolutionary algorithm)
mvpart (multivariate CART)
knnTree (nearest-neighbor-based trees)
RWeka (J4.8, M50, LMT)
LogicReg (Logic Regression)
BayesTree
TWIX (with extra splits)
party (conditional inference trees, model-based trees)

Forests in R:

randomForest(CART-based random forests)
randomSurvivalForest(for censored responses)
party(conditional random forests)
gbm(tree-based gradient boosting)
mboost(model-based and tree-based gradient boosting)


####MÉTRICAS DE EVALUACIÓN

Algorithms for constructing decision trees usually work top-down, by choosing a variable at each step that best splits the set of items.[14] Different algorithms use different metrics for measuring "best". These generally measure the homogeneity of the target variable within the subsets. Some examples are given below. These metrics are applied to each candidate subset, and the resulting values are combined (e.g., averaged) to provide a measure of the quality of the split.

* Curva ROC
* Confusion matrix
* Performance
* Classification Error rate

* **Ajuste de Bonferroni**: In statistics, the Bonferroni correction is a method used to counteract the problem of multiple comparisons. 
    The Bonferroni correction is an adjustment made to P values when several dependent or independent statistical tests are being performed simultaneously on a single data set. To perform a Bonferroni correction, divide the critical P value (α) by the number of comparisons being made. For example, if 10 hypotheses are being tested, the new critical P value would be α/10. The statistical power of the study is then calculated based on this modified P value.
    
    The Bonferroni correction is used to reduce the chances of obtaining false-positive results (type I errors) when multiple pair wise tests are performed on a single set of data. Put simply, the probability of identifying at least one significant result due to chance increases as more hypotheses are tested.

* **Gini index** (medida de desigualdad, missclassification)(low): Gini measurement is the probability of a random sample being classified correctly if we randomly pick a label according to th distribution in a branch. Measure of total variance acrros K classes. I would take this approach if I wanted to minimise missclassification probability. 

Used by the CART (classification and regression tree) algorithm, Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset. Gini impurity can be computed by summing the probability f_i of each item being chosen times the probability 1-f_i of a mistake in categorizing that item. It reaches its minimum (zero) when all cases in the node fall into a single target category.


Cuanta gente voy perdiendo a medida que avanzo en mi rama/camino/branch. La pureza del dato.
http://stats.stackexchange.com/questions/95839/gini-decrease-and-gini-impurity-of-children-nodes

i(n)=1−p2l−p2r

So, for this example with 110 observations on a node:

- node (110)
   - left (100)
      - left_left (60)
      - left_right (40)
   - right (10)
      - right_left (5)
      - right_right (5)
      
I would calculate the Gini decrease for node like this :

i(left) =1−(60/100)^2−(40/100)^2 = 0.48
i(right)= 1−(5/10)^2−(5/10)^2 = 0.50    (el más impuro, más algo GINI impurity)
i(node)= 1−(100/110)^2−(10/110)^2 =0.16



* **Cross entropy** (low)
Entropy is a measurement of information (or rather lack thereof). You calculate the information gain by making a split. If I am doing an exploratory data analysis, I would prefer the information gain so that I maximise mutual information with my tree.
In machine learning, this concept can be used to define a preferred sequence of attributes to investigate to most rapidly narrow down/branch the state of X. Such a sequence (which depends on the outcome of the investigation of previous attributes at each stage) is called a decision tree. Usually an attribute with high mutual information should be preferred to other attributes.

**For the data subset that is 100% homogeneous (all Yes or all No), the entropy of the target variable is zero and for a subset that is a perfect 50-50 mixture, entropy of the target is 1.0.**

The attribute which maximizes the gain ratio or information gain the most is selected as the root node or top node. The value of the attribute at which this gain occurs is obviously the split point. Rank ordering the gain will yield the other nodes or attributes where splitting the data resulted in increased information gain.

This partitioning may continue along the same attribute to yield smaller and smaller subsets of higher "purity". 

Cantidad de información mutua, cuanto más declinado hacia un lado u otro más fácil será para nuestro modelo. En un decisión tree el top nodo, el de más arriba es que el mayor gain ration tiene, porque es el que mayor particiona el dato. Después la partición continua según los niveles de information gain.
Muy claro: http://www.simafore.com/blog/bid/94454/A-simple-explanation-of-how-entropy-fuels-a-decision-tree-model

Gini index and cross-entropy will take on a small value if the mth node is pure.
http://www.simafore.com/blog/bid/94454/A-simple-explanation-of-how-entropy-fuels-a-decision-tree-model

* **OOB error** (out-of-bag error) 
Random forests technique involves sampling of the input data with replacement (bootstrap sampling). In this sampling, about **one thrird of the data is not used for training and can be used to testing**.These are called the out of bag samples. Error estimated on these out of bag samples is the out of bag error. 
https://www.quora.com/What-is-the-out-of-bag-error-in-Random-Forests







Bibliografía:

http://scg.sdsu.edu/ctrees_r/
http://scg.sdsu.edu/rf_r/
http://trevorstephens.com/post/72923766261/titanic-getting-started-with-r-part-3-decision


Advantages of different classification algortihms:
https://www.quora.com/What-are-the-advantages-of-different-classification-algorithms


````{r, cache=TRUE, eval=TRUE}

### SAMPLE DATA
data = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data",
        sep=",",header=F,col.names=c("age", "type_employer", "fnlwgt", "education", 
                "education_num","marital", "occupation", "relationship", "race","sex",
                "capital_gain", "capital_loss", "hr_per_week","country", "income"),
        fill=FALSE,strip.white=T)


variable <- data$num_pages



###########################



### BBVA DATA (datos sobre navegación cliente, intentamos estimar el número de paginas visitadas (num_pages) a través de otras variables continuas : plazos, importes, gastos, dias utimo acceso)

# LOAD data

#data <- read.csv("/Users/alaguna/Desktop/Ana/Scripts_machinelearning_statistics/DATO_juguete/dataset_navegacionweb_prestamo_10K.csv", header=TRUE)
attach(data)

#variable <- num_pages


library(caTools)

set.seed(88)
split <- sample.split(data, SplitRatio = 0.75)



dresstrain <- subset(data, split == TRUE)
dresstest <- subset(data, split == FALSE)


````
####1. Decision tree

Un árbol de decisión1 es un modelo de predicción utilizado en el ámbito de la inteligencia artificial. Dada una base de datos se fabrican diagramas de construcciones lógicas, muy similares a los sistemas de predicción basados en reglas, que sirven para representar y categorizar una serie de condiciones que ocurren de forma sucesiva, para la resolución de un problema.

STEPS:



#####1. Grow the Tree
rpart(formula, data=, method=,control=) where:

    * formula  is in the format 
    outcome ~ predictor1+predictor2+predictor3+ect.
    * data=	specifies the data frame
    * method=	"class" for a classification tree 
              "anova" for a regression tree
    * control=	optional parameters for controlling tree growth. For example, control=rpart.control(minsplit=30, cp=0.001) requires that the minimum number of observations in a node be 30 before attempting a split and that a split must decrease the overall lack of fit by a factor of 0.001 (cost complexity factor) before being attempted.


`````{r, cache=TRUE, echo=TRUE, eval=TRUE}

### DECISION TREE

library(rpart)
mycontrol = rpart.control(cp = 0, xval = 10)

#fittree = rpart(ind_lead_put ~  des_last_touch_channel + ind_home + ind_contenido_put + ind_simulador_put + cod_finalidad, method = "class",    data = dresstrain, control = mycontrol)

fittree = rpart(income ~  ., method = "class",
     data = dresstrain, control = mycontrol)

#library(tree)
#income_b <- ifelse(data$income=="<=50K", 0, 1)
#tree.data = tree(income_b ~  type_employer, education, race, capital_loss, capital_gain, education_num, marital, occupation, country, relationship, data = dresstrain)
#summary(tree.data)
``````


#####2. Examine the results
The following functions help us to examine the results:

    * printcp(fit)  display cp table
    * plotcp(fit)	plot cross-validation results
    * rsq.rpart(fit)	plot approximate R-squared and relative error for different splits (2 plots). labels are only appropriate for the "anova" method.
    * print(fit)	print results
    * summary(fit)	detailed results including surrogate splits
    * plot(fit)	plot decision tree
    * text(fit)	label the decision tree plot
    * post(fit, file=)	create postscript plot of decision tree

`````{r, cache=TRUE}

#### RESULTS


library(rpart)

printcp(fittree)
plotcp(fittree) 
#print(fittree)
#summary(fittree)
plot(fittree)
post(fittree)

````

#####3. Prune tree

Prune back the tree to **avoid overfitting** the data. Typically, you will want to select a tree size that minimizes the cross-validated error, the xerror column printed by printcp( ).
Prune the tree to the desired size using prune(fit, cp= )
Specifically, use printcp( ) to examine the cross-validated error results, select the complexity parameter associated with minimum error, and place it into the prune( ) function. Alternatively, you can use the code fragment

fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"]

to automatically select the complexity parameter associated with the smallest cross-validated error


````{r,cache=TRUE}



#We look for the tree with the highest cross-validated error less than the minimum cross-validated error plus the standard deviation of the error at that tree.
fittree$cptable
printcp(fittree)

# manually: cptarg = sqrt(fittree$cptable[7,1]*fittree$cptable[8,1])
# automatically the less error:
cptarg= fittree$cptable[which.min(fittree$cptable[,"xerror"]),"CP"]


prunedtree = prune(fittree,cp=cptarg)


#importancia variables
library(caret)
varImp(fittree)

prunedtree$variable.importance

prunedtree$numresp

par(mfrow = c(1, 1), mar = c(1, 1, 1, 1))
plot(prunedtree, uniform = T, compress = T, 
     margin = 0.1, branch = 0.3)
text(prunedtree, use.n = T, digits = 3, cex = 0.6)


`````

#####4.Evaluation

`````{r,cache=TRUE}

### EVALUATION

fit.preds = predict(prunedtree,newdata=dresstest,type="class")
fit.table = table(dresstest$income,fit.preds)
fit.table

accuracy=sum(diag(fit.table))/sum(fit.table)


library(ROCR)
fit.pr = predict(prunedtree,newdata=dresstest,type="prob")[,2]
fit.pred = prediction(fit.pr,dresstest$income)
fit.perf = performance(fit.pred,"tpr","fpr")
plot(fit.perf,lwd=2,col="blue",
     main= paste0("ROC:  Classification Trees - AUC: ", accuracy))
abline(a=0,b=1)


library(rattle)
library(rpart.plot)
library(RColorBrewer)

fancyRpartPlot(prunedtree)

````


####2. Random forest

`````{r, cache=TRUE, echo=TRUE, eval=TRUE}

#RANDOM FOREST

library(randomForest)
library(ROCR)


#Next we’ll need to find the optimal numbers of variables to try splitting on at each node.
bestmtry <- tuneRF(dresstrain[-15],dresstrain$income, ntreeTry=100, 
     stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE, dobest=FALSE)

## mtry= indicates that X all predictos should be considered for each split of the tree.- in other words, the bagging should be done.
## OOB error

adult.rf <-randomForest(income~.,data=dresstrain, mtry=2, ntree=1000, 
     keep.forest=TRUE, importance=TRUE,test=dresstest)

adult.rf

#prediction
adult.rf.pr = predict(adult.rf,type="prob",newdata=dresstest)[,2]

#Confusion matrix y accuracy
fit.preds = predict(prunedtree,newdata=dresstest,type="class")
fit.table = table(dresstest$income,ifelse(adult.rf.pr >= 0.5, ">50k", "<=50K"))
fit.table

accuracy=sum(diag(fit.table))/sum(fit.table)


## ROC curve
adult.rf.pred = prediction(adult.rf.pr, dresstest$income)
adult.rf.perf = performance(adult.rf.pred,"tpr","fpr")
plot(adult.rf.perf,main=paste0("ROC Random Forest - ACU:",accuracy),col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")




library(randomForest)
importance(adult.rf)
varImpPlot(adult.rf)


`````

Hemos comparado la ejecución de este data set en decision tree y en random forest y la accuracy es mejor que la de la regresión logística.
Ver Classification_LogisticRegression.Rmd




####BBVA DATA


--------------


###### BBVA DATA (datos sobre navegación cliente, intentamos estimar el número de paginas visitadas (num_pages) a través de otras variables continuas : plazos, importes, gastos, dias utimo acceso)

`````{r,cache=TRUE, eval=TRUE}
# LOAD data

data <- read.csv("/Users/alaguna/Desktop/Ana/Scripts_machinelearning_statistics/DATO_juguete/dataset_navegacionweb_prestamo_10K.csv", header=TRUE)
attach(data)

#variable <- num_pages

library(caTools)

set.seed(88)
split <- sample.split(data, SplitRatio = 0.75)

dresstrain <- subset(data, split == TRUE)
dresstest <- subset(data, split == FALSE)


````
#####1. Grow the tree

`````{r, cache=TRUE, echo=TRUE, eval=TRUE}

### DECISION TREE

library(rpart)
mycontrol = rpart.control(cp = 0, xval = 10)

#fittree = rpart(ind_lead_put ~  des_last_touch_channel + ind_home + ind_contenido_put + ind_simulador_put + cod_finalidad, method = "class",    data = dresstrain, control = mycontrol)

fittree = rpart(ind_lead_put ~  ., method = "class",
     data = dresstrain, control = mycontrol)

#library(tree)
#income_b <- ifelse(data$income=="<=50K", 0, 1)
#tree.data = tree(income_b ~  type_employer, education, race, capital_loss, capital_gain, education_num, marital, occupation, country, relationship, data = dresstrain)
#summary(tree.data)
``````


#####2.Results
`````{r, cache=TRUE}

#### RESULTS


library(rpart)

printcp(fittree)
plotcp(fittree) 
#print(fittree)
#summary(fittree)
plot(fittree)
post(fittree)

````

#####3. Prune tree


````{r,cache=TRUE}



#We look for the tree with the highest cross-validated error less than the minimum cross-validated error plus the standard deviation of the error at that tree.
fittree$cptable
printcp(fittree)

# manually: cptarg = sqrt(fittree$cptable[7,1]*fittree$cptable[8,1])
# automatically the less error:
cptarg= fittree$cptable[which.min(fittree$cptable[,"xerror"]),"CP"]


prunedtree = prune(fittree,cp=cptarg)


#importancia variables
library(caret)
varImp(fittree)

prunedtree$variable.importance

prunedtree$numresp

par(mfrow = c(1, 1), mar = c(1, 1, 1, 1))
plot(prunedtree, uniform = T, compress = T, 
     margin = 0.1, branch = 0.3)
text(prunedtree, use.n = T, digits = 3, cex = 0.6)


`````

#####4.Evaluation

`````{r,cache=TRUE}

### EVALUATION

fit.preds = predict(prunedtree,newdata=dresstest,type="class")
fit.table = table(dresstest$ind_lead_put,fit.preds)
fit.table

accuracy=sum(diag(fit.table))/sum(fit.table)


library(ROCR)
fit.pr = predict(prunedtree,newdata=dresstest,type="prob")[,2]
fit.pred = prediction(fit.pr,dresstest$ind_lead_put)
fit.perf = performance(fit.pred,"tpr","fpr")
plot(fit.perf,lwd=2,col="blue",
     main= paste0("ROC:  Classification Trees - AUC: ", accuracy))
abline(a=0,b=1)


library(rattle)
library(rpart.plot)
library(RColorBrewer)

fancyRpartPlot(prunedtree)

````


####2. Random forest

`````{r, cache=TRUE, echo=TRUE, eval=FALSE}

#RANDOM FOREST

library(randomForest)
library(ROCR)


#Next we’ll need to find the optimal numbers of variables to try splitting on at each node.
bestmtry <- tuneRF(dresstrain[-15],dresstrain$ind_lead_put, ntreeTry=100, 
     stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE, dobest=FALSE)



## mtry= indicates that X all predictos should be considered for each split of the tree.- in other words, the bagging should be done.
## OOB error

adult.rf <-randomForest(ind_lead_put~.,data=dresstrain, mtry=2, ntree=1000, 
     keep.forest=TRUE, importance=TRUE,test=head(dresstest,100))

adult.rf



#prediction
adult.rf.pr = predict(adult.rf,type="prob",newdata=head(dresstest,100))[,2]



#Confusion matrix y accuracy
fit.preds = predict(prunedtree,newdata=dresstest,type="class")
fit.table = table(dresstest$ind_lead_put,ifelse(adult.rf.pr >= 0.5, "0", "1"))
fit.table

accuracy=sum(diag(fit.table))/sum(fit.table)


## ROC curve
adult.rf.pred = prediction(adult.rf.pr, dresstest$ind_lead_put)
adult.rf.perf = performance(adult.rf.pred,"tpr","fpr")
plot(adult.rf.perf,main=paste0("ROC Random Forest - ACU:",accuracy),col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")




library(randomForest)
importance(adult.rf)
varImpPlot(adult.rf)


`````


------------



#### COMPARING TREE BASED METHODS: Decision trees, Random forests, Bagging & Boosting

`````{r, cache=TRUE, echo=TRUE, eval=FALSE}

#### KAGGLE COMPETITION: 

#http://www.r-bloggers.com/comparing-tree-based-classification-methods-via-the-kaggle-otto-competition/

# Clear environment workspace
#rm(list=ls())

# Load data
#train <- read.csv("/Users/Ana/Desktop/ClasesMATEMATICAS_GioAcademia2015/kaggle_otto-master/data/train.csv")
#test <- read.csv("/Users/Ana/Desktop/ClasesMATEMATICAS_GioAcademia2015/kaggle_otto-master/data/test.csv")
#samplesub <- read.csv("/Users/Ana/Desktop/ClasesMATEMATICAS_GioAcademia2015/kaggle_otto-master/data/sampleSubmission.csv")

#Mac

train <- read.csv("/Users/alaguna/Desktop/Ana/Scripts_machinelearning_statistics/kaggle_otto-master/data/train.csv")
test <- read.csv("/Users/alaguna/Desktop/Ana/Scripts_machinelearning_statistics/kaggle_otto-master/data/test.csv")
samplesub <- read.csv("/Users/alaguna/Desktop/Ana/Scripts_machinelearning_statistics/kaggle_otto-master/data/sampleSubmission.csv")

# Remove id column so it doesn't get picked up by the current classifier
train <- train[,-1]
summary(train)
summary(test)
# Create sample train and test datasets for prototyping new models from the train dataset
strain <- train[sample(nrow(train), 6000, replace=FALSE),]
stest <- train[sample(nrow(train), 2000, replace=FALSE),]
`````

# Basic Decision Tree

`````{r, cache=TRUE, echo=TRUE, eval=FALSE}
# Install tree package
install.packages('tree')
library(tree)
# Set a unique seed number so you get the same results everytime you run the below model,
# the number does not matter
set.seed(12)
# Create a decision tree model using the target field as the response and all 93 features as inputs (.)
fit1 <- tree(as.factor(target) ~ ., data=strain)
# Plot the decision tree
plot(fit1)
title(main="tree")
text(fit1)
# Test the tree model on the holdout test dataset
fit1.pred <- predict(fit1, stest, type="class")
# Create a confusion matrix of predictions vs actuals
table(fit1.pred,stest$target)
# Determine the error rate for the model
fit1$error <- 1-(sum(fit1.pred==stest$target)/length(stest$target))
fit1$error

#Solo predice la mitad de clases en el ?rbol, problema de overfitting y poor model.





# The decision tree model from the tree package didn't perform very well, let's try the rpart decision tree model
# Install rpart package
install.packages('rpart')
library(rpart)
# Set a unique seed number so you get the same results everytime you run the below model,
# the number does not matter
set.seed(13)
# Create a decision tree model using the target field as the response and all 93 features as inputs (.)
fit2 <- rpart(as.factor(target) ~ ., data=strain, method="class")
# Plot the decision tree
par(xpd=TRUE)
plot(fit2, compress=TRUE)
title(main="rpart")
text(fit2)
# Test the rpart (tree) model on the holdout test dataset
fit2.pred <- predict(fit2, stest, type="class")
# Create a confusion matrix of predictions vs actuals
table(fit2.pred,stest$target)
# Determine the error rate for the model
fit2$error <- 1-(sum(fit2.pred==stest$target)/length(stest$target))
fit2$error

# Neither of the decision tree models are working very well, not all of the classes are being predicted. This is probably
# because there are too many features that recursive binary partitioning is over-simplifying the model and missing some of 
# the classes completely.

`````


# Bagging

`````{r, cache=TRUE, echo=TRUE, eval=FALSE}

# Install adabag package
install.packages('adabag')
library(adabag)
# Set a unique seed number so you get the same results everytime you run the below model,
# the number does not matter
set.seed(14)
# Begin recording the time it takes to create the model
ptm3 <- proc.time()
# Create a bagging model using the target field as the response and all 93 features as inputs (.)
fit3 <- bagging(target ~ ., data=strain, mfinal=50)
# Finish timing the model
fit3$time <- proc.time() - ptm3
# Test the baggind model on the holdout test dataset
fit3.pred <- predict(fit3, stest, newmfinal=50)
# Create a confusion matrix of predictions vs actuals
table(as.factor(fit3.pred$class),stest$target)
# Determine the error rate for the model
fit3$error
`````


# Random Forest

`````{r, cache=TRUE, echo=TRUE, eval=FALSE}

# Install randomForest package
install.packages('randomForest')
library(randomForest)
# Set a unique seed number so you get the same results everytime you run the below model,
# the number does not matter
set.seed(16)
# Use the tuneRF function to determine an ideal value for the mtry parameter
mtry <- tuneRF(strain[,1:93], strain[,94], mtryStart=1, ntreeTry=50, stepFactor=2, improve=0.05,
               trace=TRUE, plot=TRUE, doBest=FALSE)
# The ideal mtry value was found to be 8
# Begin recording the time it takes to create the model
ptm4 <- proc.time()
# Create a random forest model using the target field as the response and all 93 features as inputs (.)
fit4 <- randomForest(as.factor(target) ~ ., data=strain, importance=TRUE, ntree=100, mtry=8)
# Finish timing the model
fit4.time <- proc.time() - ptm4
# Create a dotchart of variable/feature importance as measured by a Random Forest
varImpPlot(fit4)
# Test the randomForest model on the holdout test dataset
fit4.pred <- predict(fit4, stest, type="response")
# Create a confusion matrix of predictions vs actuals
table(fit4.pred,stest$target)
# Determine the error rate for the model
fit4$error <- 1-(sum(fit4.pred==stest$target)/length(stest$target))
fit4$error


``````


# Boosting

`````{r, cache=TRUE, echo=TRUE, eval=FALSE}

# Install gbm package
install.packages('gbm')
library(gbm)
# Set a unique seed number so you get the same results everytime you run the below model,
# the number does not matter
set.seed(17)
# Begin recording the time it takes to create the model
ptm5 <- proc.time()
# Create a random forest model using the target field as the response and all 93 features as inputs (.)
fit5 <- gbm(target ~ ., data=strain, distribution="multinomial", n.trees=1000, 
            shrinkage=0.05, interaction.depth=12, cv.folds=2)
# Finish timing the model
fit5.time <- proc.time() - ptm5
# Test the boosting model on the holdout test dataset
trees <- gbm.perf(fit5)
fit5.stest <- predict(fit5, stest, n.trees=trees, type="response")
fit5.stest <- as.data.frame(fit5.stest)
names(fit5.stest) <- c("Class_1","Class_2","Class_3","Class_4","Class_5","Class_6","Class_7","Class_8","Class_9")
fit5.stest.pred <- rep(NA,2000)
for (i in 1:nrow(stest)) {
  fit5.stest.pred[i] <- colnames(fit5.stest)[(which.max(fit5.stest[i,]))]}
fit5.pred <- as.factor(fit5.stest.pred)
# Create a confusion matrix of predictions vs actuals
table(fit5.pred,stest$target)
# Determine the error rate for the model
fit5$error <- 1-(sum(fit5.pred==stest$target)/length(stest$target))
fit5$error
`````



Conclusion

I expected that the basic tree models probably wouldn’t perform that well. I had no idea that some of the ensemble methods would also perform so poorly. The clear **winner from the data above is the random forest model**. Even the boosting model in its current state can’t really be used because it doesn’t make any Class_4 predictions.