Ensembling methods: Bagging & Boosting
========================

###Importante:

* **Bagging** Creating multiple copies of the original training data set using the bootstrap with replacement, fitting the algorithm separatly to each copy, and then combining all the outputs in order to create a single predictive model. Notably, each algorithme is built on a bootstrap data set, independent of the others algorithms. Around 2/3 of the observations during the training. The remaining 1/3 not used in the bagging is call Out-of-bag Error.

   - **Boostrapping** El término bootstrap se refiere a la realización de inferencias estadísticas basadas en el remuestreo de un conjunto de datos procedentes de una muestra aleatoria. http://www.expansion.com/diccionario-economico/bootstrap.html

* **Boosting** Aprendizaje con remuestreo aleatorio con reposición, it works in a similar way, except that the algoritms are grown sequentially. In the specific case of decision trees, each tree is grown using information from previously grown trees. Boosting does not involve booststrap sampling; instead each tree is fit on a modified version of the original data set. 
We train a tree using the current residuals, rather than the outcome Y, as the response.


#####Difference between boosting and bagging:

Boosting and bagging are similar, in that they are both ensembling techniques, where a number of weak learners (classifiers/regressors that are barely better than guessing) combine (through averaging or max vote) to create a strong learner that can make accurate predictions. Bagging means that you take bootstrap samples (with replacement and only 2/3 of the training data) of your data set and each sample trains a (potentially) weak learner (then the avg of the all outputs is calculated). Boosting, on the other hand, uses all data to train each learner, but instances that were misclassified by the previous learners are given more weight so that subsequent learners give more focus to them during training.



#### COMPARING TREE BASED METHODS: Decision trees, Random forests, Bagging & Boosting

`````{r, echo=TRUE, eval=FALSE}

#### KAGGLE COMPETITION: 

#http://www.r-bloggers.com/comparing-tree-based-classification-methods-via-the-kaggle-otto-competition/

# Clear environment workspace
#rm(list=ls())

# Load data
#train <- read.csv("/Users/Ana/Desktop/ClasesMATEMATICAS_GioAcademia2015/kaggle_otto-master/data/train.csv")
#test <- read.csv("/Users/Ana/Desktop/ClasesMATEMATICAS_GioAcademia2015/kaggle_otto-master/data/test.csv")
#samplesub <- read.csv("/Users/Ana/Desktop/ClasesMATEMATICAS_GioAcademia2015/kaggle_otto-master/data/sampleSubmission.csv")

#Mac

train <- read.csv("/Users/alaguna/Desktop/Ana/Scripts_machinelearning_statistics/kaggle_otto-master/data/train.csv")
test <- read.csv("/Users/alaguna/Desktop/Ana/Scripts_machinelearning_statistics/kaggle_otto-master/data/test.csv")
samplesub <- read.csv("/Users/alaguna/Desktop/Ana/Scripts_machinelearning_statistics/kaggle_otto-master/data/sampleSubmission.csv")

# Remove id column so it doesn't get picked up by the current classifier
train <- train[,-1]
summary(train)
summary(test)
# Create sample train and test datasets for prototyping new models from the train dataset
strain <- train[sample(nrow(train), 6000, replace=FALSE),]
stest <- train[sample(nrow(train), 2000, replace=FALSE),]
`````

# Basic Decision Tree

`````{r, echo=TRUE, eval=FALSE}
# Install tree package
install.packages('tree')
library(tree)
# Set a unique seed number so you get the same results everytime you run the below model,
# the number does not matter
set.seed(12)
# Create a decision tree model using the target field as the response and all 93 features as inputs (.)
fit1 <- tree(as.factor(target) ~ ., data=strain)
# Plot the decision tree
plot(fit1)
title(main="tree")
text(fit1)
# Test the tree model on the holdout test dataset
fit1.pred <- predict(fit1, stest, type="class")
# Create a confusion matrix of predictions vs actuals
table(fit1.pred,stest$target)
# Determine the error rate for the model
fit1$error <- 1-(sum(fit1.pred==stest$target)/length(stest$target))
fit1$error

#Solo predice la mitad de clases en el ?rbol, problema de overfitting y poor model.





# The decision tree model from the tree package didn't perform very well, let's try the rpart decision tree model
# Install rpart package
install.packages('rpart')
library(rpart)
# Set a unique seed number so you get the same results everytime you run the below model,
# the number does not matter
set.seed(13)
# Create a decision tree model using the target field as the response and all 93 features as inputs (.)
fit2 <- rpart(as.factor(target) ~ ., data=strain, method="class")
# Plot the decision tree
par(xpd=TRUE)
plot(fit2, compress=TRUE)
title(main="rpart")
text(fit2)
# Test the rpart (tree) model on the holdout test dataset
fit2.pred <- predict(fit2, stest, type="class")
# Create a confusion matrix of predictions vs actuals
table(fit2.pred,stest$target)
# Determine the error rate for the model
fit2$error <- 1-(sum(fit2.pred==stest$target)/length(stest$target))
fit2$error

# Neither of the decision tree models are working very well, not all of the classes are being predicted. This is probably
# because there are too many features that recursive binary partitioning is over-simplifying the model and missing some of 
# the classes completely.

`````


# Bagging

`````{r, echo=TRUE, eval=FALSE}

# Install adabag package
install.packages('adabag')
library(adabag)
# Set a unique seed number so you get the same results everytime you run the below model,
# the number does not matter
set.seed(14)
# Begin recording the time it takes to create the model
ptm3 <- proc.time()
# Create a bagging model using the target field as the response and all 93 features as inputs (.)
fit3 <- bagging(target ~ ., data=strain, mfinal=50)
# Finish timing the model
fit3$time <- proc.time() - ptm3
# Test the baggind model on the holdout test dataset
fit3.pred <- predict(fit3, stest, newmfinal=50)
# Create a confusion matrix of predictions vs actuals
table(as.factor(fit3.pred$class),stest$target)
# Determine the error rate for the model
fit3$error
`````


# Random Forest

`````{r, echo=TRUE, eval=FALSE}

# Install randomForest package
install.packages('randomForest')
library(randomForest)
# Set a unique seed number so you get the same results everytime you run the below model,
# the number does not matter
set.seed(16)
# Use the tuneRF function to determine an ideal value for the mtry parameter
mtry <- tuneRF(strain[,1:93], strain[,94], mtryStart=1, ntreeTry=50, stepFactor=2, improve=0.05,
               trace=TRUE, plot=TRUE, doBest=FALSE)
# The ideal mtry value was found to be 8
# Begin recording the time it takes to create the model
ptm4 <- proc.time()
# Create a random forest model using the target field as the response and all 93 features as inputs (.)
fit4 <- randomForest(as.factor(target) ~ ., data=strain, importance=TRUE, ntree=100, mtry=8)
# Finish timing the model
fit4.time <- proc.time() - ptm4
# Create a dotchart of variable/feature importance as measured by a Random Forest
varImpPlot(fit4)
# Test the randomForest model on the holdout test dataset
fit4.pred <- predict(fit4, stest, type="response")
# Create a confusion matrix of predictions vs actuals
table(fit4.pred,stest$target)
# Determine the error rate for the model
fit4$error <- 1-(sum(fit4.pred==stest$target)/length(stest$target))
fit4$error


``````


# Boosting

`````{r, echo=TRUE, eval=FALSE}

# Install gbm package
install.packages('gbm')
library(gbm)
# Set a unique seed number so you get the same results everytime you run the below model,
# the number does not matter
set.seed(17)
# Begin recording the time it takes to create the model
ptm5 <- proc.time()
# Create a random forest model using the target field as the response and all 93 features as inputs (.)
fit5 <- gbm(target ~ ., data=strain, distribution="multinomial", n.trees=1000, 
            shrinkage=0.05, interaction.depth=12, cv.folds=2)
# Finish timing the model
fit5.time <- proc.time() - ptm5
# Test the boosting model on the holdout test dataset
trees <- gbm.perf(fit5)
fit5.stest <- predict(fit5, stest, n.trees=trees, type="response")
fit5.stest <- as.data.frame(fit5.stest)
names(fit5.stest) <- c("Class_1","Class_2","Class_3","Class_4","Class_5","Class_6","Class_7","Class_8","Class_9")
fit5.stest.pred <- rep(NA,2000)
for (i in 1:nrow(stest)) {
  fit5.stest.pred[i] <- colnames(fit5.stest)[(which.max(fit5.stest[i,]))]}
fit5.pred <- as.factor(fit5.stest.pred)
# Create a confusion matrix of predictions vs actuals
table(fit5.pred,stest$target)
# Determine the error rate for the model
fit5$error <- 1-(sum(fit5.pred==stest$target)/length(stest$target))
fit5$error
`````





Conclusion

I expected that the basic tree models probably wouldn’t perform that well. I had no idea that some of the ensemble methods would also perform so poorly. The clear **winner from the data above is the random forest model**. Even the boosting model in its current state can’t really be used because it doesn’t make any Class_4 predictions.